1.1 Objective:
Create pipeline that reads data from varied files in a Cloud Storage bucket and writes the data to a Bigtable table
Compartmentalize the data contents in the form of logical views applicable to content sensitive consumers
Visualize the data through various reports to fulfill business requirements

1.2 Scenarios / Use Cases:
Load CSV Files from Google Cloud Storage to BigQuery Using Dataflow(data pipeline should auto-import) and Visualize the same with Data Studio

1.3 Pre-requesites:
The BigQuery dataset and table(empty) must exist.
The input csv files must exist in a Cloud Storage bucket before running the pipeline.
Data Studio API must be enabled

1.4 Driver program for the above scenario works as follows:(Refer: App.java)
1.Process each Element of csv file seperated by comma 
2.Bigtable expects a specific schema from the input csv files.
3.Pipeline will read from files from source file path you gave that is gcs ,then it will process each files then it will write it to Bigquery table
4.Run as a streaming job as the requirement is to auto import everytime when there is a new file detected in gcs.

1.5Visualizing BigQuery data using Data Studio:
Google Data Studio to visualize data in BigQuery using the BigQuery connector
1.In Data Studio env create a Blank Report.
2.Add Data -> select BigQuery as a Connector.
3.Authorize Google Data Studio access to your Google Cloud project.
4.Select a bigquery dataset and Table that you want to visualize data from


Steps:
1.Create source and temporary cloud storage buckets.
2.Create bigquery dataset and empty table.
3.Create a maven project to run your apache beam driver program
      mvn archetype:generate -DgroupId=com.bachinalabs \
        -DartifactId=gcp-pipeline \
        -DarchetypeArtifactId=maven-archetype-quickstart \
        -DarchetypeVersion=1.4 \
        -DinteractiveMode=false
4.Write your pipeline driver program & Run it
      // clean the project
      mvn clean
      // package
      mvn package
      // run the pipeline(run the jar file)
      java -jar target/gcp-pipeline-1.1-SNAPSHOT.jar --gcpTempLocation=gs://df-temp-gcp-training-01-303001/bq --runner=DataflowRunner --project=gcp-training-01-303001 --region=us-central1 --network=vino-vpc --subnetwork=regions/us-central1/subnetworks/subnet-a --streaming=true

5.Check the data gets updated in bigquery table.
6.Visualize bigquery table data with Data Studio(Refer:1.5)

