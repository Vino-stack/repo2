The pipeline consists of the following steps:

1.CSV files are uploaded to the FILES_SOURCE Cloud Storage bucket.
2.This event triggers the streaming Cloud Function.
3.Data is parsed and inserted into BigQuery.

Pre-requesites:
Ensure Cloud Functions, and Cloud Storage APIs are enabled from APIs and Services console.

Steps:
1.Create Function ,select 

Trigger type 
cloud storage

Event type
Finalize/Create

with Entrypoint : com.example.Example

  1.1 Write the funtion code for your job(Example.java)
  1.2 add dependacies in pom.xml (libraries-bom,google-cloud-storage[1.117.1],google-cloud-bigquery[1.136.0],google-cloud-logging)
  
2.Deploy the Cloud Function

3.Trigger the function by uploading csv file to source gcs bucket.

4.check the desired result in big query table
















gcloud functions logs read function-1


Reference:

Java:
https://cloud.google.com/bigquery/docs/loading-data-cloud-storage-csv#java

https://cloud.google.com/functions/docs/first-java(http trigger)

https://cloud.google.com/bigquery/docs/schema-detect#java

https://cloud.google.com/bigquery/docs/datasets

https://cloud.google.com/bigquery/docs/tables

Python:
https://medium.com/nerd-for-tech/ingest-data-from-gcs-to-bq-using-cloud-functions-abaca92514bf

https://cloud.google.com/architecture/streaming-data-from-cloud-storage-into-bigquery-using-cloud-functions#architecture
